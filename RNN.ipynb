{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Dataset using RNN, LSTM, and GRU Models\n",
    "\n",
    "This project implements and compares three types of Recurrent Neural Networks (RNN): **SimpleRNN**, **LSTM (Long Short-Term Memory)**, and **GRU (Gated Recurrent Unit)** for sentiment analysis on the **IMDB movie review dataset**. \n",
    "\n",
    "## Steps:\n",
    "1. **Data Loading**: The IMDB dataset is loaded using TensorFlow's `imdb.load_data()` method, restricting the vocabulary to the top 10,000 most frequent words.\n",
    "2. **Data Preprocessing**: The sequences of words are padded to a maximum length of 500 to ensure consistent input size.\n",
    "3. **Model Building**: A function is created to build models using different RNN cells (`SimpleRNN`, `LSTM`, `GRU`). The models use an embedding layer, followed by the RNN cell, and a dense layer with a sigmoid activation for binary sentiment classification.\n",
    "4. **Training**: The models are trained for 10 epochs using the Adam optimizer, binary cross-entropy loss, and accuracy as the evaluation metric. Early stopping is applied to prevent overfitting.\n",
    "5. **Evaluation**: The models' performance is evaluated on the test set, and the loss and accuracy are printed.\n",
    "\n",
    "## Objective:\n",
    "The goal of this project is to explore the performance of various RNN architectures (SimpleRNN, LSTM, GRU) in predicting sentiment (positive or negative) of movie reviews. The evaluation includes training on a set of preprocessed reviews, followed by testing the models' generalization ability on unseen data.\n",
    "\n",
    "## Expected Outcome:\n",
    "- Compare the test accuracy of SimpleRNN, LSTM, and GRU models.\n",
    "- Identify which RNN architecture performs best for sentiment analysis on the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n",
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 71ms/step - accuracy: 0.6013 - loss: 0.6451 - val_accuracy: 0.8100 - val_loss: 0.4371\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 75ms/step - accuracy: 0.7408 - loss: 0.5441 - val_accuracy: 0.8094 - val_loss: 0.4366\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 76ms/step - accuracy: 0.8871 - loss: 0.2878 - val_accuracy: 0.8074 - val_loss: 0.4364\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 85ms/step - accuracy: 0.9406 - loss: 0.1702 - val_accuracy: 0.8204 - val_loss: 0.4602\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - accuracy: 0.9783 - loss: 0.0786 - val_accuracy: 0.8290 - val_loss: 0.5164\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8269 - loss: 0.5203\n",
      "Test Loss: 0.5225266218185425 - Test Accuracy: 0.8274800181388855\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 10000\n",
    "MAXLEN = 500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=MAXLEN)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=MAXLEN)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "\n",
    "def build_model(rnn_cell):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "        rnn_cell(32),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_model(tf.keras.layers.SimpleRNN)\n",
    "\n",
    "lstm_model = build_model(tf.keras.layers.LSTM)\n",
    "\n",
    "gru_model = build_model(tf.keras.layers.GRU)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "history = rnn_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results = rnn_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results[0]} - Test Accuracy: {results[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Training and Evaluation\n",
    "\n",
    "In this section, the **LSTM (Long Short-Term Memory)** model is trained and evaluated for sentiment analysis on the **IMDB dataset**. The LSTM model is one of the most widely used RNN architectures, known for its ability to learn long-term dependencies in sequential data.\n",
    "\n",
    "### Training:\n",
    "- **Epochs**: The model is trained for 10 epochs, meaning it will iterate over the training data 10 times.\n",
    "- **Batch Size**: A batch size of 64 is used, meaning that the model will update its weights after processing 64 samples at a time.\n",
    "- **Validation Split**: 20% of the training data is used for validation during training, allowing for real-time monitoring of the model's performance on unseen data.\n",
    "- **Early Stopping**: Early stopping is applied to prevent overfitting. If the validation loss does not improve after 2 epochs, training will stop.\n",
    "\n",
    "### Evaluation:\n",
    "After training the model, it is evaluated on the **test set** to assess its performance in terms of loss and accuracy. The evaluation metrics are:\n",
    "- **Loss**: The binary cross-entropy loss, which measures how well the model predicts the sentiment (positive or negative).\n",
    "- **Accuracy**: The accuracy metric, which shows the percentage of correct predictions made by the model on the test data.\n",
    "\n",
    "### Output:\n",
    "The results of the evaluation (loss and accuracy) are printed out, allowing you to compare the performance of the LSTM model with other RNN models such as SimpleRNN and GRU.\n",
    "\n",
    "```python\n",
    "Test Loss: {results_LSTM[0]} - Test Accuracy: {results_LSTM[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 109ms/step - accuracy: 0.6747 - loss: 0.5941 - val_accuracy: 0.8462 - val_loss: 0.3558\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 115ms/step - accuracy: 0.8864 - loss: 0.2870 - val_accuracy: 0.8726 - val_loss: 0.3172\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 123ms/step - accuracy: 0.9241 - loss: 0.2112 - val_accuracy: 0.8790 - val_loss: 0.3055\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.9449 - loss: 0.1550 - val_accuracy: 0.8704 - val_loss: 0.3379\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 123ms/step - accuracy: 0.9607 - loss: 0.1179 - val_accuracy: 0.8704 - val_loss: 0.3490\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8645 - loss: 0.3683\n",
      "Test Loss: 0.36148661375045776 - Test Accuracy: 0.865559995174408\n"
     ]
    }
   ],
   "source": [
    "history_LSTM = lstm_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_LSTM = lstm_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_LSTM[0]} - Test Accuracy: {results_LSTM[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model Training and Evaluation\n",
    "\n",
    "In this section, the **GRU (Gated Recurrent Unit)** model is trained and evaluated for sentiment analysis on the **IMDB dataset**. The GRU model is another type of recurrent neural network (RNN) designed to solve the vanishing gradient problem in traditional RNNs, while being computationally more efficient than LSTMs.\n",
    "\n",
    "### Training:\n",
    "- **Epochs**: The model is trained for 10 epochs, meaning it iterates over the training data 10 times.\n",
    "- **Batch Size**: A batch size of 64 is used, meaning that the model updates its weights after processing 64 samples at a time.\n",
    "- **Validation Split**: 20% of the training data is used for validation during training, allowing for real-time monitoring of the model's performance on unseen data.\n",
    "- **Early Stopping**: Early stopping is applied to prevent overfitting. If the validation loss does not improve after 2 epochs, training stops early.\n",
    "\n",
    "### Evaluation:\n",
    "After training, the model is evaluated on the **test set** to measure its performance in terms of loss and accuracy. The evaluation metrics are:\n",
    "- **Loss**: The binary cross-entropy loss, which measures how well the model predicts sentiment (positive or negative).\n",
    "- **Accuracy**: The accuracy metric, which shows the percentage of correct predictions made by the model on the test data.\n",
    "\n",
    "### Output:\n",
    "The results of the evaluation (loss and accuracy) are printed, allowing you to compare the performance of the GRU model with other RNN models such as SimpleRNN and LSTM.\n",
    "\n",
    "```python\n",
    "Test Loss: {results_GRU[0]} - Test Accuracy: {results_GRU[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 128ms/step - accuracy: 0.6628 - loss: 0.5784 - val_accuracy: 0.8442 - val_loss: 0.3581\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 148ms/step - accuracy: 0.8946 - loss: 0.2744 - val_accuracy: 0.8684 - val_loss: 0.3231\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.8569 - loss: 0.3382\n",
      "Test Loss: 0.33401742577552795 - Test Accuracy: 0.8593599796295166\n"
     ]
    }
   ],
   "source": [
    "history_GRU = gru_model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_GRU = gru_model.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_GRU[0]} - Test Accuracy: {results_GRU[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model with Attention\n",
    "\n",
    "In this section, we extend the basic **LSTM (Long Short-Term Memory)** model by incorporating an **Attention Mechanism** to improve the model's ability to focus on important parts of the input sequence for sentiment analysis on the **IMDB dataset**.\n",
    "\n",
    "### Model Architecture:\n",
    "1. **Input Layer**: The input layer accepts sequences of integer-encoded words, each sequence having a maximum length of 500 words (`MAXLEN`).\n",
    "2. **Embedding Layer**: The embedding layer is used to convert the integer-encoded words into dense vectors of fixed size (32 dimensions). The vocabulary size is set to 10,000 (`VOCAB_SIZE`).\n",
    "3. **LSTM Layer**: An **LSTM** layer is used to process the input sequence and capture long-term dependencies in the data. The number of units in the LSTM is set to 32 (`rnn_units`), and it outputs the full sequence (due to `return_sequences=True`).\n",
    "4. **Attention Mechanism**: The attention layer is applied to the output of the LSTM layer. It allows the model to focus on important parts of the sequence when making predictions. The attention mechanism computes a weighted sum of the inputs, which helps the model better understand the context of each word in the sequence.\n",
    "5. **Flatten Layer**: The attention output is flattened to a one-dimensional vector.\n",
    "6. **Dense Layer**: The final dense layer with a sigmoid activation function produces a binary output (sentiment: positive or negative).\n",
    "\n",
    "### Training Configuration:\n",
    "- **Optimizer**: Adam optimizer is used to minimize the binary cross-entropy loss, which is commonly used for binary classification tasks.\n",
    "- **Loss Function**: Binary cross-entropy loss, appropriate for binary classification problems like sentiment analysis.\n",
    "- **Metrics**: Accuracy is used as the evaluation metric to measure the percentage of correct predictions.\n",
    "\n",
    "### Model Summary:\n",
    "The model summary is printed to show the architecture of the model, including the number of parameters at each layer and the overall model.\n",
    "\n",
    "### Next Steps:\n",
    "This model can be trained on the IMDB dataset (or any other sequence-based data) by fitting it to the training data and evaluating its performance on the test data. The addition of the attention mechanism should improve the model's performance, especially in handling long sequences with varying levels of importance in different parts of the input.\n",
    "\n",
    "```python\n",
    "lstm_model_with_attention.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">320,000</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,001</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │    \u001b[38;5;34m320,000\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m8,320\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m16,001\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">344,321</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m344,321\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">344,321</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m344,321\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAXLEN = 500\n",
    "\n",
    "def build_model2(rnn_cell, rnn_units=32):\n",
    "    sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedded_sequences = Embedding(VOCAB_SIZE, rnn_units)(sequence_input)\n",
    "    rnn_output = rnn_cell(rnn_units, return_sequences=True)(embedded_sequences)\n",
    "\n",
    "    query_value_attention_seq = tf.keras.layers.Attention()([rnn_output, rnn_output])\n",
    "\n",
    "    attention_flatten = Flatten()(query_value_attention_seq)\n",
    "\n",
    "    dense_output = Dense(1, activation='sigmoid')(attention_flatten)\n",
    "\n",
    "    model = Model(inputs=sequence_input, outputs=dense_output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# LSTM Model with Attention\n",
    "lstm_model_with_attention = build_model2(tf.keras.layers.LSTM)\n",
    "lstm_model_with_attention.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of LSTM Model with Attention\n",
    "\n",
    "In this section, the **LSTM model with Attention** is trained on the **IMDB dataset** to perform sentiment analysis and evaluate its performance on a test set.\n",
    "\n",
    "### Steps:\n",
    "1. **Model Training**:\n",
    "   The model is trained using the `fit` method for 10 epochs with a batch size of 64. A validation split of 20% is used, meaning that 20% of the training data is set aside for validation during training. The **EarlyStopping callback** is used to stop the training early if the validation loss does not improve, which prevents overfitting.\n",
    "\n",
    "   ```python\n",
    "   history_ = lstm_model_with_attention.fit(input_train, y_train,\n",
    "                       epochs=10,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       validation_split=0.2,\n",
    "                       callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 195ms/step - accuracy: 0.6152 - loss: 0.6075 - val_accuracy: 0.8664 - val_loss: 0.3283\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 190ms/step - accuracy: 0.9004 - loss: 0.2542 - val_accuracy: 0.8912 - val_loss: 0.2830\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 193ms/step - accuracy: 0.9355 - loss: 0.1692 - val_accuracy: 0.8896 - val_loss: 0.3176\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 197ms/step - accuracy: 0.9534 - loss: 0.1196 - val_accuracy: 0.8890 - val_loss: 0.3553\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.8812 - loss: 0.3617\n",
      "Test Loss: 0.36264434456825256 - Test Accuracy: 0.8790000081062317\n"
     ]
    }
   ],
   "source": [
    "history_ = lstm_model_with_attention.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[callback])\n",
    "\n",
    "results_ = lstm_model_with_attention.evaluate(input_test, y_test)\n",
    "print(f'Test Loss: {results_[0]} - Test Accuracy: {results_[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
